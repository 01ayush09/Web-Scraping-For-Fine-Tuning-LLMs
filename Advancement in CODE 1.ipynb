{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7781629",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced NLP Project: End‑to‑End Text Classification (Scrape → Dataset → Experiments → Tuning → Evaluation → Demo)\n",
    "\n",
    "**Last updated:** 2025-09-04 09:33\n",
    "\n",
    "This notebook upgrades a real‑world NLP pipeline to an **advanced, resume‑ready** project. It includes:\n",
    "\n",
    "- **Data ingestion**: respectful web scraping & content extraction\n",
    "- **Dataset building**: cleaning, labeling, and Hugging Face `datasets` integration\n",
    "- **EDA**: quick sanity checks, length stats, and class balance plots\n",
    "- **Modeling**: multiple transformer backbones (DistilBERT, BERT, RoBERTa)\n",
    "- **Training**: Hugging Face `Trainer` with early stopping, mixed precision\n",
    "- **Hyperparameter tuning**: `optuna` via `Trainer.hyperparameter_search`\n",
    "- **Evaluation**: accuracy, precision, recall, F1 (macro), confusion matrix, error analysis\n",
    "- **Inference & Packaging**: pipeline for batch inference\n",
    "- **Deployment**: minimal **Gradio** demo\n",
    "- **Reproducibility**: config cell, seeds, and model card stub\n",
    "\n",
    "> Tip: Run sections incrementally. Comment/uncomment heavy cells (tuning/SHAP) if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49747a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%capture\n",
    "# If running fresh, uncomment installs.\n",
    "# !pip install -U transformers datasets evaluate accelerate scikit-learn optuna gradio bs4 trafilatura matplotlib pandas numpy\n",
    "# Optional (heavy): shap umap-learn\n",
    "# !pip install shap umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e43992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, time, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "                          EarlyStoppingCallback, pipeline, set_seed)\n",
    "\n",
    "# Optional: optuna for tuning\n",
    "import optuna\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef838d",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data Ingestion: Scraping / Import\n",
    "\n",
    "Two options:\n",
    "\n",
    "1. **Scrape**: provide a list of URLs for each class (distant supervision).  \n",
    "2. **Import**: load a CSV with columns `text` and `label`.\n",
    "\n",
    "> For resume-quality, keep a small **URL manifest** per class and a **raw dump** CSV for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac73f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import trafilatura\n",
    "\n",
    "def fetch_text(url, timeout=15):\n",
    "    \"\"\"Fetch and extract main text from a web page using trafilatura (fallback to BeautifulSoup).\"\"\"\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url, no_ssl=True, timeout=timeout)\n",
    "        if downloaded:\n",
    "            text = trafilatura.extract(downloaded, include_formatting=False, include_images=False)\n",
    "            if text and len(text.split()) > 50:\n",
    "                return text\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Fallback: simple BS4 (less accurate)\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for tag in soup(['script','style','header','footer','nav','aside']):\n",
    "                tag.decompose()\n",
    "            text = ' '.join(soup.get_text(separator=' ').split())\n",
    "            if len(text.split()) > 50:\n",
    "                return text\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def crawl_class(urls, label, sleep=1.0):\n",
    "    records = []\n",
    "    for u in urls:\n",
    "        txt = fetch_text(u)\n",
    "        if txt:\n",
    "            records.append({\"url\": u, \"text\": txt, \"label\": label})\n",
    "        time.sleep(sleep)  # be polite\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# EXAMPLE: Provide your own URLs per label (keep small for demo)\n",
    "URLS = {\n",
    "    # \"tech\": [\"https://example.com/article1\", \"https://example.com/article2\"],\n",
    "    # \"sports\": [\"https://example.com/article3\"],\n",
    "}\n",
    "\n",
    "DO_SCRAPE = False  # set True after filling URLS above\n",
    "\n",
    "if DO_SCRAPE and URLS:\n",
    "    frames = []\n",
    "    for label, urls in URLS.items():\n",
    "        frames.append(crawl_class(urls, label))\n",
    "    df_raw = pd.concat(frames, ignore_index=True)\n",
    "else:\n",
    "    # Fallback demo: create a tiny synthetic dataset (replace with your CSV or scraping output)\n",
    "    data = {\n",
    "        \"text\": [\n",
    "            \"Apple unveils new chips for AI on-device computing at the developer conference.\",\n",
    "            \"The local team clinched the championship after a dramatic penalty shootout.\",\n",
    "            \"Researchers propose a novel transformer variant that improves long-context modeling.\",\n",
    "            \"A record-breaking run highlights the athlete's training regimen and endurance.\"\n",
    "        ],\n",
    "        \"label\": [\"tech\", \"sports\", \"tech\", \"sports\"],\n",
    "        \"url\": [None, None, None, None]\n",
    "    }\n",
    "    df_raw = pd.DataFrame(data)\n",
    "\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db947fc2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Cleaning & Label Normalization\n",
    "- Drop duplicates/empties\n",
    "- Normalize labels to consecutive integers with a mapping (stored in metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str): return None\n",
    "    # Lightweight cleaning; avoid aggressive normalization that might hurt models\n",
    "    s = s.strip()\n",
    "    s = ' '.join(s.split())\n",
    "    return s if len(s.split()) >= 5 else None\n",
    "\n",
    "df = df_raw.copy()\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df = df.dropna(subset=['text', 'label']).drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "labels = sorted(df['label'].unique())\n",
    "label2id = {lbl:i for i,lbl in enumerate(labels)}\n",
    "id2label = {i:lbl for lbl,i in label2id.items()}\n",
    "\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Counts:\\n\", df['label'].value_counts())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6382e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train/Validation/Test Split & Hugging Face Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=SEED, stratify=df['label_id'])\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df['label_id'])\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[['text','label_id']]),\n",
    "    \"validation\": Dataset.from_pandas(valid_df[['text','label_id']]),\n",
    "    \"test\": Dataset.from_pandas(test_df[['text','label_id']]),\n",
    "})\n",
    "\n",
    "num_labels = len(labels)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f5c87",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Quick EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b49c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lengths = [len(t.split()) for t in df['text']]\n",
    "plt.figure()\n",
    "plt.hist(lengths, bins=20)\n",
    "plt.title(\"Text length distribution (words)\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Class balance\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67232e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_CANDIDATES = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "]\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_length)\n",
    "\n",
    "# Prepare a default tokenizer to inspect\n",
    "default_tokenizer = AutoTokenizer.from_pretrained(MODEL_CANDIDATES[0])\n",
    "tokenized_ds_preview = ds[\"train\"].select(range(min(5, ds[\"train\"].num_rows))).map(lambda x: tokenize_function(x, default_tokenizer))\n",
    "tokenized_ds_preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35776b78",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Metrics\n",
    "We report **accuracy**, **precision**, **recall**, **F1 (macro)** and the **confusion matrix**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36848418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision_macro\": pr, \"recall_macro\": rc, \"f1_macro\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10053d42",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Training Utilities\n",
    "Trains a model for a few epochs with early stopping and returns the `Trainer` and evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e261f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one(model_name, ds, num_labels, output_dir, learning_rate=2e-5, batch_size=8, epochs=3):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenized = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=[\"text\"])\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        seed=SEED,\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        report_to=[\"none\"],\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate(tokenized[\"validation\"])\n",
    "    return trainer, eval_metrics\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1722d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Baseline Experiments (Multiple Backbones)\n",
    "Run a short training for each candidate backbone and compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "trainers = {}\n",
    "\n",
    "for model_name in MODEL_CANDIDATES:\n",
    "    out_dir = f\"models/{model_name.replace('/', '_')}-baseline\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    trainer, metrics = train_one(model_name, ds, num_labels, out_dir, learning_rate=2e-5, batch_size=8, epochs=3)\n",
    "    metrics[\"model\"] = model_name\n",
    "    results.append(metrics)\n",
    "    trainers[model_name] = trainer\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1ca9d",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Hyperparameter Tuning (Optuna)\n",
    "We run a small hyperparameter search on the best backbone to squeeze extra performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_name = df_results.iloc[0][\"model\"]\n",
    "print(\"Best baseline backbone:\", best_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_name, use_fast=True)\n",
    "tokenized = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=[\"text\"])\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"models/{best_model_name.replace('/', '_')}-tuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    seed=SEED,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 6),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "    }\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=6\n",
    ")\n",
    "\n",
    "best_run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a481938",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Final Training & Test Evaluation\n",
    "Train with the best-found hyperparameters on train+validation and evaluate on the held-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01faa254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply best hyperparameters\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "# Merge train+validation for final training\n",
    "train_val = datasets.concatenate_datasets([tokenized[\"train\"], tokenized[\"validation\"]])\n",
    "\n",
    "trainer.train_dataset = train_val\n",
    "trainer.train()\n",
    "\n",
    "test_metrics = trainer.evaluate(tokenized[\"test\"])\n",
    "print(json.dumps(test_metrics, indent=2))\n",
    "\n",
    "# Confusion matrix & report\n",
    "preds = trainer.predict(tokenized[\"test\"]).predictions.argmax(axis=-1)\n",
    "true = np.array(tokenized[\"test\"][\"label_id\"])\n",
    "\n",
    "cm = confusion_matrix(true, preds, labels=list(range(num_labels)))\n",
    "print(\"\\nClassification report:\\n\", classification_report(true, preds, target_names=labels, zero_division=0))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(range(num_labels), labels, rotation=45, ha='right')\n",
    "plt.yticks(range(num_labels), labels)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a8047",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Error Analysis (Most-Confused Examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = test_df.reset_index(drop=True).copy()\n",
    "df_test['pred'] = preds\n",
    "df_test['true_label'] = df_test['label_id'].map(id2label)\n",
    "df_test['pred_label'] = df_test['pred'].map(id2label)\n",
    "errors = df_test[df_test['pred'] != df_test['label_id']]\n",
    "\n",
    "# Show a few hardest errors (longest texts among misclassified or any heuristic)\n",
    "errors_sorted = errors.sort_values(by=df_test['text'].str.len(), ascending=False, na_position='last')\n",
    "errors_sorted[['text','true_label','pred_label']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1d937",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Inference Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = pipeline(\"text-classification\", model=trainer.model, tokenizer=trainer.tokenizer, return_all_scores=True, truncation=True)\n",
    "samples = [\n",
    "    \"The player scored a hat-trick in the final match.\",\n",
    "    \"A breakthrough in quantum processors was announced by the research team.\"\n",
    "]\n",
    "predictions = clf(samples)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a43e4",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Save Model & Minimal Gradio Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5095522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = \"deployable_model\"\n",
    "trainer.model.save_pretrained(save_dir)\n",
    "trainer.tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def predict_gradio(text):\n",
    "    res = clf(text)[0]\n",
    "    # Return label with max score\n",
    "    best = max(res, key=lambda x: x['score'])\n",
    "    return f\"{best['label']} ({best['score']:.3f})\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict_gradio,\n",
    "    inputs=gr.Textbox(lines=4, label=\"Enter text\"),\n",
    "    outputs=gr.Textbox(label=\"Prediction\"),\n",
    "    title=\"NLP Classifier Demo\",\n",
    "    description=\"Transformer-based text classifier with tuned hyperparameters.\",\n",
    ")\n",
    "\n",
    "# To launch locally:\n",
    "# demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186fc81",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Model Card (Stub)\n",
    "\n",
    "**Model**: `<best_backbone>-finetuned-topic-classifier`  \n",
    "**Labels**: {label → id} mapping stored in notebook.  \n",
    "**Intended Use**: Topic classification of news-like short/medium texts.  \n",
    "**Training Data**: URLs listed in manifest or CSV; see `df_raw`.  \n",
    "**Metrics**: Accuracy, Precision, Recall, F1 (macro), Confusion Matrix.  \n",
    "**Ethical Considerations**: Beware of dataset bias, domain drift, and potential misclassification harms.  \n",
    "**Limitations**: Small dataset, limited topics; not robust to slang, code-mixed text without further finetuning.  \n",
    "**How to Reproduce**: Run this notebook top-to-bottom after filling URL manifests or loading your CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta = {\n",
    "    \"labels\": labels,\n",
    "    \"label2id\": label2id,\n",
    "    \"id2label\": id2label,\n",
    "    \"seed\": SEED,\n",
    "    \"candidates\": MODEL_CANDIDATES,\n",
    "    \"best_backbone\": best_model_name,\n",
    "    \"test_metrics\": test_metrics,\n",
    "}\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "with open(\"artifacts/metadata.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\"Saved artifacts/metadata.json\"\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

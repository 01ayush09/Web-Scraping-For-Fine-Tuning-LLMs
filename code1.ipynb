{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# changes are only done in\n",
        "# 1. url = \" \"\n",
        "# 2. hf_token = ' '"
      ],
      "metadata": {
        "id": "Jxfh_yfWBY0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ3nbP8o_MYY"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "W7pLRTlF_Num"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl(url_to_crawl):\n",
        "  url = \"\"\n",
        "\n",
        "  payload = {\n",
        "        \"url\": url_to_crawl\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "      \"accept\": \"application/json\",\n",
        "      \"content-type\": \"application/json\",\n",
        "      \"authorization\": \"Basic VTAwMDAyNjc0Mzk6UFdfMTczYzkwYmM2ZTY1MDRhMDU0MjY4ODg5MTZlODlhYzEz\"\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "_jaZMpFx_R8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article_text(article_url):\n",
        "    try:\n",
        "      # Crawl article\n",
        "      crawled_article = crawl(article_url)\n",
        "      crawled_article_json = json.loads(crawled_article.text)\n",
        "\n",
        "      # If article is not crawled correctly retutn None\n",
        "      status_code = crawled_article_json['results'][0]['status_code']\n",
        "      if status_code!=200:\n",
        "        return None\n",
        "\n",
        "      # Create BeautifulSoup object from HTML\n",
        "      html_string = crawled_article_json['results'][0]['content']\n",
        "      soup = BeautifulSoup(html_string,'html.parser')\n",
        "\n",
        "      # Get Article Text\n",
        "      story_div = soup.find('div',id='storytext')\n",
        "      if story_div is None:\n",
        "        return None\n",
        "\n",
        "      text = story_div.get_text(strip=True,separator='\\n')\n",
        "\n",
        "      return text\n",
        "    except:\n",
        "      return None"
      ],
      "metadata": {
        "id": "MCo6x_ZH_Szv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_article(category_url,batch_size=10):\n",
        "  start_index=1\n",
        "  while True:\n",
        "    # Crawl Index Page\n",
        "    crawled_page = crawl(f\"{category_url}?start={start_index}&count={batch_size}\")\n",
        "    crawled_page_json = json.loads(crawled_page.text)\n",
        "\n",
        "    # Break out if pages are finished\n",
        "    status_code = crawled_page_json['results'][0]['status_code']\n",
        "    if status_code!=200:\n",
        "      break\n",
        "\n",
        "    # Create BeautifulSoup objects for Index page\n",
        "    html_string = crawled_page_json['results'][0]['content']\n",
        "    soup = BeautifulSoup(html_string,'html.parser')\n",
        "\n",
        "    # Loop over each article in Index Page\n",
        "    for article in soup.find_all('article'):\n",
        "      # Get Article text\n",
        "      anchor_tag = article.find('a')\n",
        "      if anchor_tag is None:\n",
        "        continue\n",
        "\n",
        "      article_url = anchor_tag['href']\n",
        "      atricle_text = get_article_text(article_url)\n",
        "\n",
        "       # Skip Articles that had issues\n",
        "      if atricle_text is None:\n",
        "        continue\n",
        "\n",
        "      yield atricle_text\n",
        "\n",
        "    start_index+=batch_size"
      ],
      "metadata": {
        "id": "vxES0KQq_XnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls_to_crawl = {\n",
        "    \"politics\":\"https://www.npr.org/get/1014/render/partial/next\", #?start=11&count=20\n",
        "    \"business\":\"https://www.npr.org/get/1006/render/partial/next\",\n",
        "    \"health\":\"https://www.npr.org/get/1128/render/partial/next\",\n",
        "    \"science\":\"https://www.npr.org/get/1007/render/partial/next\",\n",
        "    \"climate\":\"https://www.npr.org/get/1167/render/partial/next\",\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "lPTtggV8_Ypx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[]\n",
        "for news_category,category_url in urls_to_crawl.items():\n",
        "  print(f\"Crawling {news_category}\")\n",
        "  articles_crawled=0\n",
        "\n",
        "  for article_text in get_next_article(category_url):\n",
        "    data.append({'news_category':news_category,'article':article_text})\n",
        "    articles_crawled+=1\n",
        "\n",
        "    if articles_crawled%100==0:\n",
        "      print(f\"Crawled {articles_crawled} articles\")\n",
        "\n",
        "    if articles_crawled>=1000:\n",
        "      break\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(f\"news_articles_dataset_{news_category}.csv\",index=False)"
      ],
      "metadata": {
        "id": "WjQZibbq_bv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "vGN6aedc_edO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"news_articles_dataset.csv\",index=False)"
      ],
      "metadata": {
        "id": "ctDpUTj2_iQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import huggingface_hub"
      ],
      "metadata": {
        "id": "W22C34Fh_lRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Parameters\n",
        "dataset_csv_path='news_articles_dataset.csv'\n",
        "text_column_name = 'article'\n",
        "label_column_name = 'news_category'\n",
        "test_size=0.2\n",
        "num_labels = 2 # Default is 2, it's going to be overwritten after reading data\n",
        "\n",
        "# Model Parameters\n",
        "model_name='meta-llama/Llama-3.2-1B'\n",
        "hf_token=''"
      ],
      "metadata": {
        "id": "SrKcXlzq_mLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('news_articles_dataset.csv')\n",
        "num_labels = df['news_category'].nunique()"
      ],
      "metadata": {
        "id": "vy2fG8t8_qr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_hub.login(hf_token)"
      ],
      "metadata": {
        "id": "8TnrHbim_tn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "metadata": {
        "id": "YP_uO1wH_uYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cleaner():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def remove_hml_tags(self,text):\n",
        "    clean_text = BeautifulSoup(text,'lxml').text\n",
        "    return clean_text\n",
        "  def remove_double_spaces(self,text):\n",
        "    clean_text = re.sub(' +',' ',text)\n",
        "    return clean_text\n",
        "  def clean(self,text):\n",
        "    clean_text = self.remove_hml_tags(text)\n",
        "    clean_text = self.remove_double_spaces(clean_text)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "X4pGmv2d_xQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaner = Cleaner()\n",
        "df['text_cleaned'] = df[text_column_name].apply(cleaner.clean)"
      ],
      "metadata": {
        "id": "Xq6TJ4_h_zmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "hkl84kds_1sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df[label_column_name].tolist())\n",
        "df['label'] = le.transform(df[label_column_name].tolist())"
      ],
      "metadata": {
        "id": "vYIAIGlc_5sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "TaSAWnts_8Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train,df_test = train_test_split(df,test_size=test_size)"
      ],
      "metadata": {
        "id": "6ZNBDd03_89G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape,df_test.shape"
      ],
      "metadata": {
        "id": "4hpFp4eZAAtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[['text_cleaned','label']]\n",
        "df_test = df_test[['text_cleaned','label']]"
      ],
      "metadata": {
        "id": "JiijWq7BADHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "wbPZ13hWAGzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "test_dataset = Dataset.from_pandas(df_test)"
      ],
      "metadata": {
        "id": "V8rIXxcwAHqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "_VPb8pRyAKC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer= AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "tq4FjY0XAMY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "C-exfoJ2AQns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "  return tokenizer(examples['text_cleaned'],truncation=True)"
      ],
      "metadata": {
        "id": "tvA1nCq-ARWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = train_dataset.map(preprocess_function,batched=True)\n",
        "tokenized_test = test_dataset.map(preprocess_function,batched=True)"
      ],
      "metadata": {
        "id": "FHF55usWATeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "loU6Q_qDAVzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=num_labels)"
      ],
      "metadata": {
        "id": "Gs6PijzoAYSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "id": "DckSXTWaAajP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_layers =0\n",
        "for param in model.base_model.parameters():\n",
        "  number_of_layers+=1\n",
        "print(f\"Number of layers: {number_of_layers}\")"
      ],
      "metadata": {
        "id": "9hGIm9BDAcwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_number =0\n",
        "for param in model.base_model.parameters():\n",
        "  if layer_number>= number_of_layers-25:\n",
        "    break\n",
        "  number_of_layers+=1\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "id": "WOibqN7jAfCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments,Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iGYWZzGcAhTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator=  DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Evn0KIIxAjn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load('accuracy')\n",
        "def compute_metrics(eval_pred):\n",
        "  logits,labels = eval_pred\n",
        "  predictions = np.argmax(logits,axis=-1)\n",
        "  return metric.compute(predictions=predictions,references=labels)"
      ],
      "metadata": {
        "id": "d6ivIaHPAmAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=10,\n",
        "\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    save_steps=2000\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "HWlbuZ8NAorW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "vxDEwbZTAr1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label = {i: label for i, label in enumerate(le.classes_)}\n",
        "model.config.label2id = {label:i for i, label in enumerate(le.classes_)}"
      ],
      "metadata": {
        "id": "_ATipSM7AwSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('./news_classifier_model')\n",
        "tokenizer.save_pretrained('./news_classifier_model')"
      ],
      "metadata": {
        "id": "6HAw3BupAy3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save in HuggingFace hub -- Make sure to have your token to have Write acess\n",
        "model.push_to_hub(\"news-classifier-model\")\n",
        "trainer.push_to_hub(\"news-classifier-model\")\n",
        "tokenizer.push_to_hub(\"news-classifier-model\")"
      ],
      "metadata": {
        "id": "grhp1NSjA05k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "Ps6uIUEoA3FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(tokenized_train)\n",
        "preds = np.argmax(preds[:3][0],axis=-1)\n",
        "GT = df_train['label'].tolist()\n",
        "print(classification_report(GT,preds))"
      ],
      "metadata": {
        "id": "Zvbc7qo1A5b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(tokenized_test)\n",
        "preds = np.argmax(preds[:3][0],axis=-1)\n",
        "GT = df_test['label'].tolist()\n",
        "print(classification_report(GT,preds))"
      ],
      "metadata": {
        "id": "w3oRK5z8A841"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "Pp_qKjFcA9nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = pipeline(\"text-classification\",\n",
        "               model=\"AbdullahTarek/news-classifier-model\",\n",
        "               tokenizer=\"AbdullahTarek/news-classifier-model\",\n",
        "               )"
      ],
      "metadata": {
        "id": "lMAn65S5BAA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exmple_article = \"\"\"\n",
        "Naughty or nice? That's often how I think about foods packed with carbohydrates. Whole grains, like brown rice and whole wheat, fall squarely into the nice category, while white pasta and rice, well they're more naughty.\n",
        "\n",
        "\"They're naughty, in a sense, because we digest them rapidly and that creates a fast rise in blood sugar,\" says nutritionist Mindy Patterson, at Texas Woman's University in Houston. They're also low in fiber and protein, compared to their whole grain cousins.\n",
        "\n",
        "Over time, all those quick surges in blood sugar can hurt your health, Patterson says. They can contribute to insulin resistance and just leave you feeling tired.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S5Eg3JmCBCSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result= clf(exmple_article)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "nFk7YcyVBHeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}